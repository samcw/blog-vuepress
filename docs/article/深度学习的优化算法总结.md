---
title: 深度学习的优化算法总结
category: Deep Learning
description: 最近学习吴恩达老师深度学习课程，老师陆陆续续介绍了几种优化的方法，为了防止混淆，我在这片文章中做了梳理，这篇文章会随着学习的推进，不断更新。
date: 2020-03-23
---

## 深度学习优化方法总结

### 1.BGD

`Batch Gradient Descent`可能是最常见的一种优化算法，这种算法在每一次迭代当中，都是用了训练集的所有内容。

**优点**：因为他每一步都利用了训练集中的所有数据，因此当损失函数达到最小值的时候，能够保证计算出的梯度为0，也就是说，我们不需要逐渐的减小学习率。

**缺点**：每一次迭代，都会使用所有的训练集，那么如果训练集非常大，它的运行速度会变的很慢。

### 2.SGD

`Stochastic Gradient Descent`随机梯度下降，其跟`Mini-batch Gradient Descent`是同一个思路。其每次训练，都会从训练集中抽取一部分样本，来进行迭代。

这种算法在迭代过程中，需要不断的减小学习率，以使得模型较好收敛，一般学习率公式为：

$\alpha_k=\beta\alpha_{k-1}+(1-\beta)\alpha_0$

其中$\alpha_k$为第k次迭代的学习率，$\alpha_0$为原始学习率，$\beta$是一个超参数，一般设置为1%.

**优点**：由于每次训练，并不是使用所有的训练集数据，而是抽取一部分，那么它的训练速度会很快，同时由于训练集很大，收敛的速度也会比较快。

**缺点**：与此同时，虽然训练速度有所变快，但是由于每次迭代使用的不是完整的训练集，每次得到的梯度就会有误差，那么随着迭代次数增加，需要将其学习率不断减小，以防止无法收敛。

### 3.MBGD

`Mini-batch Gradient Descent`算法是对上述两种算法的折衷处理。首先为了避免迭代速度过慢，其依旧是对训练集取子集来计算梯度。但是不同于SGD，**MBGD会将整个训练集，分成若干份，遍历训练每一份数据**。

**优点**：该算法结合了以上两种算法的优点，既提高了速度，同时也充分利用了训练集中的所有数据。

**缺点**：但是该算法对学习率的选择，依然是较为困难的。

**以上3种算法，做一个总结就是**：

**Batch gradient descent:** Use all examples in each iteration；

**Stochastic gradient descent:** Use 1 example in each iteration；

**Mini-batch gradient descent:** Use b examples in each iteration.

### 4.Momentum

该方法主要是为了解决噪声过大的问题，在**面对连续但是有小幅度噪声**的情况下，尤为适合。该算法参考了指数加权平均数，将噪声给“抹平”。

![https://tva1.sinaimg.cn/large/00831rSTgy1gd3ppil28tj30k003fmxk.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3ppil28tj30k003fmxk.jpg)

在吴恩达老师的课程中，给出了其迭代更新的细节：

![https://tva1.sinaimg.cn/large/00831rSTgy1gd3qadrrt7j305r040748.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3qadrrt7j305r040748.jpg)

这里引入了超参数$\beta$，一般取值0.9。

**特点**：其不仅可以抑制非正确迭代路线方向的振荡，在我们期望的迭代方向上，还能够加速迭代。

### 5.RMSProp

`Root Mean Square Prop`算法的做法，跟Momentum中的做法类似。它引入了一个衰减系数，是的每次。

![https://tva1.sinaimg.cn/large/00831rSTgy1gd3q9tsz9xj30j702gdg4.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3q9tsz9xj30j702gdg4.jpg)

在吴恩达老师的课程中，给出了其迭代更新的细节：

![https://tva1.sinaimg.cn/large/00831rSTgy1gd3q2x5s8gj306k0480sq.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3q2x5s8gj306k0480sq.jpg)

这里的超参数$\beta$一般取值为0.99。注意更新`W、b`的时候，$S_{dW}$以及$S_{db}$位于分母出，为了防止分母为0，一般为引入一个很小的值$\epsilon$，通常取值$10^{-8}$。

**优点**：适合于处理非平稳目标。

**缺点**：引入了超参数$\beta$，同时也依赖于全局学习率$\alpha$.

### 6.Adam

Adam算法，最早是由`Diederik P. Kingma`以及`Jimmy Lei Ba`在论文中提出来的，它将上述两种算法做了整合，本质上就是带有动量项的RMSProp。

![https://tva1.sinaimg.cn/large/00831rSTgy1gd3qvct8dyj306y0avmxi.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3qvct8dyj306y0avmxi.jpg)

在论文中`《ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION》`中，分析了几种不同优化算法的效果：

1. 逻辑回归：实验一，论文采用了MNIST数据集，对784个纬度的图像矢量单元进行标签分类：

   ![https://tva1.sinaimg.cn/large/00831rSTgy1gd3qz52j87j311o0hin2f.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3qz52j87j311o0hin2f.jpg)

   结果表明，Adam算法和SGD算法表现较为接近，但是两者都要优于Adagrad算法。实际上，Adagrad算法善于处理稀疏特征的问题，由此提出了实验二。

   实验二，论文采用了将IMDB电影评论预处理为BoW特征向量的方式，来验证处理稀疏特性问题的情况下，各算法的表现。可以看出SGD算法表现较差，另外3种算法表现接近。

2. 多层神经网络：

   实验构建了一个包含2个完成隐藏层，每层有1000个隐藏单元，激活函数都为`RELU`，`mini-batch`的大小为128的对比实验。

   ![https://tva1.sinaimg.cn/large/00831rSTgy1gd3sewdx7nj30u20judka.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3sewdx7nj30u20judka.jpg)

   ​	可以看到，Adam算法相较于其他算法有着更好的收敛性。

3. 卷积神经网络：

   ![https://tva1.sinaimg.cn/large/00831rSTgy1gd3sggbfrqj31120ii78v.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3sggbfrqj31120ii78v.jpg)

   卷积神经网络的应用中，Adam算法也是要优于其他算法的。

4. 论文中还分析了各种超参数对于结果的影响，以及偏差矫正所带来的效果：

   ![https://tva1.sinaimg.cn/large/00831rSTgy1gd3smw7ueij313o0f4dim.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3smw7ueij313o0f4dim.jpg)

   图中，红色线代表对偏差矫正之后的损失曲线，绿色代表没有进行矫正的损失曲线。当$\beta_1=0$的时候，其实就是RMSProp算法，$\beta_1=0.9$就是Adam算法。在不存在偏差矫正时，$\beta_2$的取值，会对结果造成较大的影响，矫正之后这种影响会减小。

   **从图中我们可以知道，Adam的最佳实践方式就是采用偏差矫正的同时，取一个较小的$\beta_2$的值。**

   这里展示论文中，Adam算法的伪代码：

   ![https://tva1.sinaimg.cn/large/00831rSTgy1gd3tbu8gugj312a0ocn3f.jpg](https://tva1.sinaimg.cn/large/00831rSTgy1gd3tbu8gugj312a0ocn3f.jpg)

   我们可以看到，在算出了梯度的一阶矩估计以及二阶矩估计之后，Adam算法还进行了偏差矫正，这在RMSProp算法中是没有的。**之所以要进行偏差矫正，是因为$m_0v_0$的初始化值都为0，这样在一开始的时候，会产生较大的偏差**，不过在多次迭代之后，偏差会逐渐减小。为了抵消这种偏差，就有了偏差矫正的步骤。

   实际上，无论Adam算法是否进行偏差矫正，其效果都是要优于RMSProp算法的。

**最后，总结一下Adam算法，其本质上就是Momentum算法和RMSProp算法的结合，同时进行了偏差矫正。由于Adam算法的优异特性，现在一般都是使用其作为优化策略。**
